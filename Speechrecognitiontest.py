import pyaudio # Module which helps in using the microphone and recording sound inoput 
import time
import wave # Helps in converting the binary data into an voltage simulated wave of a sound wave which can be used by the speaker to prdouce sound
import whisper
from faster_whisper import WhisperModel
import torch
import subprocess

start_time = int(time.time())

language_map = {
    "en": "English",
    "hi": "Hindi",
    "mr": "Marathi",
    "fr": "French",
    "es": "Spanish",
    "de": "German",
    "zh": "Chinese",
    "ja": "Japanese",
    "ru": "Russian",
}

p = pyaudio.PyAudio() # This helps to bring all the resources related to micrphone and speaker on board, like for example if we have two microphones it would show both of them

stream = p.open(   # stream helps in opeing one of the port of the micrphone so that we can use that port as a channel to bring in input from the user
    format=pyaudio.paInt16, #format bsically helps in imporving the quality of the sound wave as it takes the data in the form of 16 bits and it is a classic form 
    channels=1, # there are two types one is stero and one is i don't remember, but i only know that during recording the standard channel is 1 
    rate=44100, # When the sound waves pass through the microphone they get converted into voltage signals, this number gives commands to our micrphone to take 44100 dots of wave per second into the inpout device(microphone)
    input=True, # input true means that we are recording not plalying the recoding using a speaker when we say output = ture it plays the recording
    frames_per_buffer=8192 # so bscially when we take 44100 bits of data in a second we don't usually acess all the data at a go what we do is we break the data into chunks or we can say packets and we are saying out of 
                           # that we take 1024 bits of data at once in a loop again and again from the total dta of 44100 bits which is obtained in 1 second, means 44100/1024 = 43 packets would be genrated in a second and if we want to record for 5 second we need 43 *5  packets 
)

time_set = int(input("How much time you want to record (in seconds): "))

frames = []  # This is the list in which we store all the data which is obtained during recording

total_samples = int(44100 * time_set)
samples_recorded = 0

while samples_recorded < total_samples:
    data = stream.read(8192)
    frames.append(data)
    samples_recorded += 8192


stream.stop_stream() # we are stopping the recording
stream.close() # we are closing it 
p.terminate() # we are giving the recourses back to the OS(Opreating system)

# Now we have the recording in a list named Frames but we have to write it into a file, but the file type needs to be .WAV so that it can be played as a recording so we use the wave module 
wf = wave.open("recorded.wav", "wb") # we open a file named X in write binary mode 
wf.setnchannels(1) # so that the wave file work as per our recording we need to give the same channels, rate, format of the sound 
wf.setsampwidth(p.get_sample_size(pyaudio.paInt16)) # same as above
wf.setframerate(44100) # same as above
wf.writeframes(b''.join(frames)) # now here what we are doing is that whenever we get a chunk of data using the sample.read(1024) there is a b'' format of the storeage so there would b'' format at every chunk of data and we need a comman b'' so bscially using the .join function we join the list using b'' so that the .WAV file doesn't get corrupted
wf.close() # after we have finished our work we just close the file so that the resources don't get wasted 

def get_cpu_name():
    try:
        result = subprocess.run(
            ['powershell', '-Command', '(Get-WmiObject -Class Win32_Processor).Name'],
            capture_output=True, text=True
        )
        cpu_name = result.stdout.strip()
        return cpu_name
    except Exception as e:
        return f"Error: {str(e)}"


def transcribe_audio_usingGPU(): # This is basically a function which can transcribe any audio means bascaly any aduio file on which the OPenAI model is trained
    
        # Load model (move this outside if you're doing multiple transcriptions)
        print('GPU:',torch.cuda.get_device_name(0))
       
        gpu_name = torch.cuda.get_device_name(0)        
        print(f"CUDA CORES ARE AVIALABLE MOVING TO {gpu_name}")
        time.sleep(1)
        print('Loading model.....')
        time.sleep(0.5)
        model = WhisperModel('medium', device='cuda', compute_type='float16')
        print("Transcribing audio...") # Now the model will read the .wav file and would understand the bluff in the auido and would use all it's permutations and combitnions to generate a proabability of the word said in the aduio the file which we are reading here is an RECORDED.WAV which is generated by the above code
        time.sleep(0.2)
        segments, info = model.transcribe("recorded.wav") # This result variable basically inputs the result and the result is in the form of a dictionary having trhee values 
                                                    # text --- means the text spoken by the person
                                                    # Language --- the model can also tell which language the person is speaking 
        language_code = info.language
        language_spoken = language_map.get(language_code, language_code)

        f = open("output_file.txt", 'a', encoding='utf-8')
        f.write(f'The language you have spoken is {language_spoken}\n')
        f.write('------------------------------------------------------------------\n')
        f.write("The transcribed text is ---->\n")
        
        for segment in segments:
            f.write(segment.text)

        f.close()
                
           
        time.sleep(0.2)
        print("Successfully saved transcription to output_file.txt")
        print("Transcription (GPU):", "done wrote it into the output file")

def transcribe_audio_usingCPU():
        
        cpu_name = get_cpu_name()
        print(f'No GPU found moving to CPU: {cpu_name}')
        time.sleep(0.3)
        print("Loading model...") # This shows that the OPENAI model is loading 
        # model = whisper.load_model("medium").to("cuda") # Here we load the model into our program which would use CUDA CORES OF THE GPU for it's proper functioning
        model = WhisperModel('medium', device='cpu', compute_type='int8')
        # Transcribe audio
        time.sleep(0.3)
        print("Transcribing audio...") # Now the model will read the .wav file and would understand the bluff in the auido and would use all it's permutations and combitnions to generate a proabability of the word said in the aduio the file which we are reading here is an RECORDED.WAV which is generated by the above code
        segments, info = model.transcribe("recorded.wav") # This result variable basically inputs the result and the result is in the form of a dictionary having trhee values 
                                                  # text --- means the text spokaen by the person
                                                  # Language --- the model can also tell which language the person is speaking 
        # text = result["text"]
        # language = result["language"]
        
        language_code = info.language
        language_spoken = language_map.get(language_code, language_code)

        f = open("output_file.txt", 'a', encoding='utf-8')
        f.write(f'The language you have spoken is {language_spoken}\n')
        f.write('------------------------------------------------------------------\n')
        f.write("The transcribed text is ---->\n")
        
        for segment in segments:
            f.write(segment.text)

        f.close()
        time.sleep(0.3)
        print("Successfully saved transcription to output_file.txt")
        print("Transcription (CPU):", "done wrote it into the output file")

        

if __name__ == "__main__":
    
    use_cuda = torch.cuda.is_available()

    if use_cuda == True:
        transcribe_audio_usingGPU()
        end_time = int(time.time())
        final_time = end_time-start_time
        language = ''
        print(f"The time taken for the program to complete is {final_time} seconds....")
    else:
        transcribe_audio_usingCPU()
        end_time = int(time.time())
        final_time = end_time-start_time
        language = ''
        print(f"The time taken for the program to complete is {final_time} seconds....")